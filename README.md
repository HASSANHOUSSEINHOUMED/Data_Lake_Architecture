# ğŸ—ï¸ Data Lake Architecture - TP Final

---

## ğŸ“Œ Contexte

**Projet** : Pipeline Data Lake complet selon l'architecture **Medallion**  
**MatiÃ¨re** : Architecture Data Lake, Data Warehouse & Data Lakehouse - MastÃ¨re 2  
**Type** : Projet en Ã©quipe (4 personnes)  
**Stack** : PySpark 4.0.1 + PostgreSQL + Parquet + Streaming Kafka  

Pipeline production-ready couvrant **ingestion** (Bronze) â†’ **transformations** (Silver) â†’ **analytics** (Gold) pour une base e-commerce complÃ¨te (Northwind).

---

## ğŸ‘¥ Ã‰quipe & Contributions

| RÃ´le | Personne | SpÃ©cialitÃ© |
|------|----------|-----------|
| **Phase 1** | Meissa | Bronze Ingestion (PostgreSQL) |
| **Phase 2** | Marcus | Silver Transformations (Dim/Fact) |
| **Phase 3-4** | Hedi | Streaming & IntÃ©gration Batch/Streaming |
| **Phase 5** ğŸ”¥ | **Hassan HOUSSEIN HOUMED** | Gold Analytics + KPIs + RFM + Dashboard |

---

## ğŸ¯ Mon RÃ´le : Data Analytics - Phase 5 (Gold Layer)

J'ai conÃ§u et implÃ©mentÃ© **la couche Gold complÃ¨te** : KPIs mÃ©tier, segmentation clients RFM et dashboard analytique.

### ğŸ’° KPIs Revenue ImplÃ©mentÃ©s

**Indicateurs clÃ©s de performance calculÃ©s** :

| KPI | Description | RÃ©sultat |
|-----|-------------|----------|
| **Chiffre d'affaires Total** | Montant total des commandes | ~$1.3M |
| **Top 10 Pays par Revenue** | RÃ©partition gÃ©ographique | USA, France, Germany en tÃªte |
| **Revenue Cumulatif** | Tendance croissante | Visualisation par pays |
| **Panier Moyen** | Ticket moyen | $1,565 |

### ğŸ‘¥ Analyse RFM - Segmentation Clients

**ModÃ¨le de segmentation 4 segments** basÃ© sur :

- **Recency** (R) : Jours depuis derniÃ¨re commande
- **Frequency** (F) : Nombre de commandes
- **Monetary** (M) : Valeur totale dÃ©pensÃ©e

**RÃ©sultats segmentation** :

| Segment | Description | Clients | StratÃ©gie |
|---------|-------------|---------|-----------|
| **VIP** | Q3+ monetary (trÃ¨s haute valeur) | ~23 | FidÃ©lisation premium |
| **LOYAL** | Q2-Q3 (fidÃ¨les rÃ©guliers) | ~23 | Rewards & upsell |
| **REGULAR** | Q1-Q2 (clients normaux) | ~23 | Engagement standard |
| **AT_RISK** | <Q1 (Ã  risque) | ~22 | Winback campaigns |

---

## ğŸ“Š Dashboard ExÃ©cutif - 6 Visualisations

J'ai crÃ©Ã© un **dashboard complet** avec Matplotlib/Seaborn :

1. **Top 10 Pays par Revenue** (Bar chart horizontal)
2. **Distribution Segmentation RFM** (Pie chart)
3. **Distribution Recency** (Histogramme)
4. **Frequency vs Monetary Value** (Scatter plot)
5. **Revenue Cumulatif** (Line chart + fill)
6. **RÃ©sumÃ© ExÃ©cutif** (Table KPIs clÃ©s)

---

## ğŸ”„ Flux de DonnÃ©es Complet (Medallion)
```
PostgreSQL (Northwind)
    â†“ [PHASE 1 - Meissa]
BRONZE (ingestion brute)
    â”œâ”€ customers (91 lignes)
    â”œâ”€ orders (830 lignes)
    â”œâ”€ order_details (2,155 lignes)
    â”œâ”€ products (77 lignes)
    â””â”€ + mÃ©tadonnÃ©es (_horodatage_ingestion, _systeme_source, etc.)
    
    â†“ [PHASE 2 - Marcus]
SILVER (transformations dimensionnelles)
    â”œâ”€ Dim_Customers (InitCap, MAJUSCULES)
    â”œâ”€ Dim_Products (stock_status, categories)
    â””â”€ Fact_Orders (montant_net calculÃ©)
    
    â†“ [PHASE 3-4 - Hedi]
STREAMING + INTEGRATION
    â”œâ”€ Kafka simulation (50 messages)
    â””â”€ Batch + Streaming merge
    
    â†“ [PHASE 5 - Hassan] ğŸ”¥
GOLD (analytics & KPIs)
    â”œâ”€ Revenue Analysis (KPIs, pays, cumulatif)
    â”œâ”€ RFM Segmentation (4 segments clients)
    â””â”€ Dashboard (6 visualisations + rÃ©sumÃ©)
```

---

## ğŸ› ï¸ Stack Technique

### **Big Data & Processing**
- ![Apache Spark](https://img.shields.io/badge/Apache%20Spark-E25A1C?style=flat-square&logo=apache-spark&logoColor=white) - Traitement distribuÃ© PySpark 4.0.1
- ![Parquet](https://img.shields.io/badge/Parquet-FF6B6B?style=flat-square) - Format columaire optimisÃ©
- ![Hadoop](https://img.shields.io/badge/Hadoop-66CCFF?style=flat-square&logo=apache-hadoop&logoColor=white) - Stockage distribuÃ©

### **Source & IntÃ©gration**
- ![PostgreSQL](https://img.shields.io/badge/PostgreSQL-336791?style=flat-square&logo=postgresql&logoColor=white) - Base source Northwind
- ![JDBC](https://img.shields.io/badge/JDBC-FF9800?style=flat-square) - Connecteur Spark-PostgreSQL
- ![Apache Kafka](https://img.shields.io/badge/Apache%20Kafka-231F20?style=flat-square&logo=apache-kafka&logoColor=white) - Streaming simulÃ©

### **Analytics & Visualization**
- ![Matplotlib](https://img.shields.io/badge/Matplotlib-1f77b4?style=flat-square) - Graphiques statiques
- ![Seaborn](https://img.shields.io/badge/Seaborn-9467bd?style=flat-square) - Visualisations statistiques
- ![Pandas](https://img.shields.io/badge/Pandas-150458?style=flat-square&logo=pandas&logoColor=white) - Manipulation DataFrames

### **Infrastructure**
- ![Docker](https://img.shields.io/badge/Docker-2496ED?style=flat-square&logo=docker&logoColor=white) - Containerization
- ![JupyterLab](https://img.shields.io/badge/JupyterLab-F37726?style=flat-square&logo=jupyter&logoColor=white) - Notebook interactif
- ![Git](https://img.shields.io/badge/Git-F05032?style=flat-square&logo=git&logoColor=white) - Version control

---

## ğŸš€ Lancement du Projet

### **DÃ©marrer l'infrastructure**
```bash
docker-compose up -d
```
â†’ Lance PostgreSQL, Spark, JupyterLab, Kafka, MinIO

### **Ouvrir le notebook**
```
http://localhost:8888
notebooks/TP_FINAL_COMPLET.ipynb
```

### **ExÃ©cuter les phases dans l'ordre**
```
Phase 1: Ingestion Bronze (7 tables)
    â†“
Phase 2: Transformations Silver (Dim/Fact)
    â†“
Phase 3-4: Streaming Kafka + IntÃ©gration
    â†“
Phase 5: Gold Analytics + RFM + Dashboard
```

---

## ğŸ’¡ DÃ©cisions Architecturales JustifiÃ©es

### **1. Architecture Medallion (Bronze â†’ Silver â†’ Gold)**

| Couche | Objectif | Avantage |
|--------|----------|----------|
| **Bronze** | Ingestion brute | Source unique de vÃ©ritÃ© (SoT) |
| **Silver** | Transformations | DonnÃ©es propres, standardisÃ©es |
| **Gold** | Analytics | PrÃªte pour consommation mÃ©tier |

âœ… **Justification** : SÃ©paration des responsabilitÃ©s, traÃ§abilitÃ© complÃ¨te, scalabilitÃ©.

### **2. MÃ©tadonnÃ©es Techniques Obligatoires**
```python
_horodatage_ingestion  # Quand
_systeme_source        # D'oÃ¹ (postgresql/kafka)
_nom_table            # Quoi
_date_ingestion       # Partitionnement
```

âœ… **Justification** : Data governance, audit trail, reproductibilitÃ©.

### **3. Parquet pour le Stockage**

| Format | Compression | RequÃªtes | Choix |
|--------|------------|----------|-------|
| CSV | Non | Lente | âŒ |
| JSON | Moyenne | Moyenne | âš ï¸ |
| **Parquet** | âœ… 5-10Ã— | âœ… Rapide | âœ… |

âœ… **Justification** : Compression, performance analytique, standard Big Data.

### **4. RFM Segmentation basÃ©e sur Quartiles**
```python
VIP     = Q3+ monetary
LOYAL   = Q2-Q3
REGULAR = Q1-Q2
AT_RISK = <Q1
```

âœ… **Justification** : Segment Ã©quilibrÃ©s, adapter stratÃ©gies marketing par groupe.

---

## ğŸ“ˆ RÃ©sultats & Impact

**DonnÃ©es traitÃ©es** :
- 7 tables ingÃ©rÃ©es (4 obligatoires + 3 bonus)
- 3,199 lignes brutes
- 2,155 lignes fact_orders aprÃ¨s transformations

**KPIs produits** :
- Chiffre d'affaires : ~$1.3M
- Clients actifs : 91
- Commandes : 830
- Taux conversion : Excellent

**Segmentation clients** :
- 4 segments RFM gÃ©nÃ©rÃ©s
- Clients VIP identifiÃ©s
- StratÃ©gies marketing diffÃ©renciÃ©es

**QualitÃ© pipeline** :
- âœ… 100% fiabilitÃ© (0 perte donnÃ©es)
- âœ… MÃ©tadonnÃ©es complÃ¨tes (traÃ§abilitÃ©)
- âœ… Format optimisÃ© (Parquet)
- âœ… Code production-ready

---

## ğŸ“ CompÃ©tences DÃ©montrÃ©es

- âœ… **Architecture Data Lake** (Medallion pattern)
- âœ… **Spark SQL & PySpark** (ingestion, transformations, agrÃ©gations)
- âœ… **Data Modeling** (Dim/Fact tables, star schema)
- âœ… **Analytics & KPIs** (Revenue, RFM, segmentation)
- âœ… **Data Visualization** (Matplotlib, Seaborn, 6 graphiques)
- âœ… **Data Governance** (mÃ©tadonnÃ©es, audit trail)
- âœ… **ETL/ELT patterns** (batch & streaming)
- âœ… **Collaboration Ã©quipe** (4 personnes, phases dÃ©pendantes)

---

## ğŸ“‚ Fichiers LivrÃ©s
```
TP_FINAL_COMPLET.ipynb   - Notebook complet (5 phases)
TP_FINAL_COMPLET.pdf     - Version PDF
README.md                - Documentation projet
```

---

## ğŸ‘¤ Auteur - Phase 5

**Hassan HOUSSEIN HOUMED**  
ğŸ“š MastÃ¨re 2 Big Data & Intelligence Artificielle - IPSSI Paris  
ğŸ“§ hassan.houssein.houmed@gmail.com  
ğŸ™ GitHub : https://github.com/HASSANHOUSSEINHOUMED

---

<div align="center">

**DerniÃ¨re mise Ã  jour** : Janvier 2026  
**MatiÃ¨re** : Architecture Data Lake, Data Warehouse & Data Lakehouse

</div>
